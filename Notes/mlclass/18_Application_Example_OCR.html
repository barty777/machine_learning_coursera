<html>
<head>
  <title>18_Application_Example_OCR</title>
  <basefont face="Tahoma" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="Evernote Windows/209040 (en-US); Windows/6.1.7601 Service Pack 1;"/>
  <link rel="stylesheet" type="text/css" href="style.css" media="all" />
</head>
<body>
<a name="2833"/>
<h1>18: Application Example OCR</h1>

<p><a href="17_Large_Scale_Machine_Learning.html">Previous</a> <a href="19_Course_Summary.html">Next</a></body></html> <a href="index.html">Index</a> </p>

<div>
<u style="font-size: 27px;"><b>Problem description and pipeline</font></b></u><ul style="font-size: 18px;"><li>Case study focused around photo OCR</font></li><li>Three reasons to do this</font><ul><li>1) Look at how a <b><font color="#1C3387">complex system</font></b> can be put together</font></li><li>2) The idea of a machine learning <b><font color="#1C3387">pipeline</font></b></font><ul><li>What to do next</font></li><li>How to do it</font></li></ul></li><li>3) Some more interesting ideas</font><ul><li>Applying machine learning to tangible problems</font></li><li><b><font color="#1C3387" face="'Times New Roman'">Artificial data synthesis</font></b></li></ul></li></ul></li></ul><b style="font-size: 18px;">What is the photo OCR problem?</font></b><div style="font-size: 18px;"><ul><li>Photo OCR = photo optical character recognition</font><ul><li>With growth of digital photography, lots of digital pictures</font></li><li>One idea which has interested many people is getting computers to understand those photos</font></li><li>The photo OCR problem is getting computers to read text in an image</font><ul><li>Possible applications for this would include</font><ul><li>Make searching easier (e.g. searching for photos based on words in them)<br></font></li><li>Car navigation</font></li></ul></li></ul></li></ul></li><li><span style="border-collapse: separate; color: rgb(0, 0, 0); font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px;">OCR of documents is a comparatively easy problem</font></span><ul><li><span style="border-collapse: separate; color: rgb(0, 0, 0); font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px;">From photos it's really hard</font></span></li></ul></li></ul><b>OCR pipeline</font></b></div><div><ul style="font-size: 18px;"><li>1) Look through image and find text</font></li><li>2) Do character segmentation </font></li><li>3) Do character classification</font></li><li>4) <i>Optional</i> some may do spell check after this too</font><ul><li>We're not focussing on such systems though</font></li></ul></li></ul><div style="font-size: 18px;"><img src="18_Application_Example_OCR_files/Image.png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></div><ul style="font-size: 18px;"><li><b><font color="#E30000">Pipelines</font></b> are common in machine learning</font><ul><li>Separate modules which may each be a machine learning component or data processing component</font></li></ul></li><li>If you're designing a machine learning system, pipeline design is one of the most important questions</font><ul><li>Performance of pipeline and each module often has a big impact on the overall performance a problem</font></li><li>You would often have different engineers working on each module</font><ul><li>Offers a natural way to divide up the workload</font></li></ul></li></ul></li></ul><b style="font-size: 27px;"><u>Sliding window image analysis</font></u></b></div><div style="font-size: 18px;"><ul><li>How do the individual models work?</font></li><li>Here focus on a sliding windows classifier</font></li></ul><ul><li>As mentioned, stage 1 is <b><font color="#E30000">text detection</font></b></font></li><li style="list-style: none; display: inline"><ul><li>Unusual problem in computer vision - different rectangles (which surround text) may have different aspect ratios (aspect ratio being height : width)</font><ul><li>Text may be short (few words) or long (many words)</font></li><li>Tall or short font</font></li><li>Text might be straight on</font></li><li>Slanted<br><img src="18_Application_Example_OCR_files/Image [1].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li></ul></li><li>Let's start with a simpler example</font></li></ul></li></ul><div><b>Pedestrian detection</font></b></div><ul><li>Want to take an image and find pedestrians in the image<b><br></b><img src="18_Application_Example_OCR_files/Image [2].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li><li>This is a slightly simpler problem because the aspect ration remains pretty constant</font></li><li>Building our detection system</font><ul><li>Have 82 x 36 aspect ratio</font><ul><li>This is a typical aspect ratio for a standing human</font></li></ul></li><li>Collect training set of positive and negative examples<br><img src="18_Application_Example_OCR_files/Image [3].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li><li>Could have 1000 - 10 000 training examples</font></li><li>Train a neural network to take an image and classify that image as pedestrian or not</font><ul><li>Gives you a way to train your system</font></li></ul></li></ul></li><li>Now we have a new image - how do we find pedestrians in it?</font><ul><li>Start by taking a rectangular 82 x 36 patch in the image<br><img src="18_Application_Example_OCR_files/Image [4].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font><ul><li>Run patch through classifier - hopefully in this example it will return y = 0</font></li></ul></li><li>Next slide the rectangle over to the right a little bit and re-run</font><ul><li>Then slide again</font></li><li>The amount you slide each rectangle over is a parameter called the step-size or stride</font><ul><li>Could use 1 pixel</font><ul><li>Best, but computationally expensive</font></li></ul></li><li>More commonly 5-8 pixels used</font></li></ul></li><li>So, keep stepping rectangle along all the way to the right</font><ul><li>Eventually get to the end</font></li></ul></li><li>Then move back to the left hand side but step down a bit too</font></li><li>Repeat until you've covered the whole image</font></li></ul></li><li>Now, we initially started with quite a small rectangle </font><ul><li>So now we can take a larger image patch (of the same aspect ratio)</font></li><li>Each time we process the image patch, we're resizing the larger patch to a smaller image, then running that smaller image through the classifier</font></li></ul></li><li>Hopefully, by changing the patch size and rastering repeatedly across the image, you eventually recognize all the pedestrians in the picture<br><img src="18_Application_Example_OCR_files/Image [5].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li></ul></li></ul><b>Text detection example</font></b></div><div style="font-size: 18px;"><ul><li>Like pedestrian detection, we generate a labeled training set with</font><ul><li>Positive examples (some kind of text) </font></li><li>Negative examples (not text)<br><img src="18_Application_Example_OCR_files/Image [6].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li></ul></li><li>Having trained the classifier we apply it to an image</font></li><li style="list-style: none; display: inline"><ul><li>So, run a sliding window classifier at a fixed rectangle size</font></li><li>If you do that end up with something like this<br><img src="18_Application_Example_OCR_files/Image [7].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/><br><img style="cursor: default;cursor: default;"></img></font></li><li>White region show where text detection system thinks text is</font><ul><li>Different shades of gray correspond to probability associated with how sure the classifier is the section contains text</font><ul><li>Black - no text</font></li><li>White - text</font></li></ul></li><li>For text detection, we want to draw rectangles around all the regions where there is text in the image</font></li></ul></li><li>Take classifier output and apply an <b><font color="#1C3387">expansion algorithm</font></b></font></li><li style="list-style: none; display: inline"><ul><li>Takes each of white regions and expands it</font></li><li>How do we implement this</font></li><li style="list-style: none; display: inline"><ul><li>Say, for every pixel, is it within some distance of a white pixel?</font></li><li>If yes then colour it white<br><img style="cursor: default;cursor: default;"></img><img src="18_Application_Example_OCR_files/Image [8].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li></ul></li></ul></li><li>Look at connected white regions in the image above</font><ul><li>Draw rectangles around those which make sense as text (i.e. tall thin boxes don't make sense)<br><img style="cursor: default;cursor: default;"></img><img src="18_Application_Example_OCR_files/Image [9].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li></ul></li><li>This example misses a piece of text on the door because the aspect ratio is wrong</font></li><li style="list-style: none; display: inline"><ul><li>Very hard to read</font></li></ul></li></ul></li></ul><b>Stage two is character segmentation</font></b></div><div style="font-size: 18px;"><ul><li>Use supervised learning algorithm</font></li><li>Look in a defined image patch and decide, is there a split between two <span style="border-collapse: separate; color: rgb(0, 0, 0); font-style: normal; font-variant: normal; font-weight: normal; letter-spacing: normal; line-height: normal; orphans: 2; text-align: -webkit-auto; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-border-horizontal-spacing: 0px; -webkit-border-vertical-spacing: 0px; -webkit-text-decorations-in-effect: none; -webkit-text-size-adjust: auto; -webkit-text-stroke-width: 0px;">characters?</span></font><ul><li>So, for example, our first training data item below looks like there is such a split<br></font></li><li>Similarly, the negative examples are either empty or hold a full characters<br><img src="18_Application_Example_OCR_files/Image [10].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/><img style="cursor: default;cursor: default;"></img></font></li></ul></li><li>We train a classifier to try and classify between positive and negative examples</font><ul><li>Run that classifier on the regions detected as containing text in the previous section</font></li></ul></li><li>Use a 1-dimensional sliding window to move along text regions<br></font><ul><li>Does each window snapshot look like the split between two characters?</font><ul><li>If yes insert a split</font></li><li>If not move on</font></li></ul></li><li>So we have something that looks like this<br><img src="18_Application_Example_OCR_files/Image [11].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/><br></font></li></ul></li></ul><b>Character classification</font></b></div><ul style="font-size: 18px;"><li>Standard OCR, where you apply standard supervised learning which takes an input and identify which character we decide it is</font></li><li style="list-style: none; display: inline"><ul><li>Multi-class characterization problem</font></li></ul></li></ul><div style="font-size: 27px;"><b><u>Getting lots of data: Artificial data synthesis</font></u></b></div><ul style="font-size: 18px;"><li>We've seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias algorithm and train on a massive data set</font><ul><li>Where do we get so much data from </font></li><li>In ML artifice data synthesis</font><ul><li>Doesn't apply to every problem</font></li><li>If it applies to your problem can be a great way to generate loads of data</font></li></ul></li></ul></li><li>Two main principles</font></li><li style="list-style: none; display: inline"><ul><li>1) Creating data from scratch</font></li><li>2) If we already have a small labeled training set can we amplify it into a larger training set</font></li></ul></li></ul><div style="font-size: 18px;"><b>Character recognition as an example of data synthesis</font></b></div><ul style="font-size: 18px;"><li>If we go and collect a large labeled data set will look like this <br><img style="cursor: default;cursor: default;"></img></font><ul><li>Goal is to take an image patch and have the system recognize the character</font></li><li>Treat the images as gray-scale (makes it a bit easer)<br><img src="18_Application_Example_OCR_files/Image [12].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li></ul></li><li>How can we amplify this</font><ul><li>Modern computers often have a big font library</font></li><li>If you go to websites, huge free font libraries</font></li><li>For more training data, take characters from different fonts, paste these characters again random backgrounds</font></li></ul></li><li>After some work, can build a synthetic training set <br><img src="18_Application_Example_OCR_files/Image [13].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/><br></font><ul><li>Random background</font></li><li>Maybe some blurring/distortion filters</font></li><li>Takes thought and work to make it look realistic</font><ul><li>If you do a sloppy job this won't help!</font></li><li>So unlimited supply of training examples</font></li></ul></li><li>This is an example of creating new data from scratch</font></li></ul></li><li>Other way is to introduce distortion into existing data</font><ul><li>e.g. take a character and warp it<br><img src="18_Application_Example_OCR_files/Image [14].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/><br></font><ul><li>16 new examples</font></li><li>Allows you amplify existing training set</font></li></ul></li><li>This, again, takes though and insight in terms of deciding how to amplify</font></li></ul></li></ul><b style="font-size: 18px;">Another example: speech recognition</font></b><ul style="font-size: 18px;"><li>Learn from audio clip - what were the words</font><ul><li>Have a labeled training example</font></li><li>Introduce audio distortions into the examples<br></font></li></ul></li><li>So only took one example</font><ul><li>Created lots of new ones!</font></li></ul></li><li>When introducing distortion, they should be reasonable relative to the issues your classifier may encounter</font></li></ul><div style="font-size: 18px;"><br></font></div><div style="font-size: 18px;"><b>Getting more data</font></b></div><ul style="font-size: 18px;"><li>Before creating new data, make sure you have a low bias classifier</font><ul><li>Plot learning curve</font></li></ul></li><li>If not a low bias classifier increase number of features</font><ul><li>Then create large artificial training set</font></li></ul></li><li>Very important question: How much work would it be to get 10x data as we currently have?</font><ul><li>Often the answer is, &quot;Not that hard&quot;</font></li><li>This is often a huge way to improve an algorithm</font></li><li>Good question to ask yourself or ask the team</font></li></ul></li><li>How many minutes/hours does it take to get a certain number of examples</font><ul><li>Say we have 1000 examples</font></li><li>10 seconds to label an example</font></li><li>So we need another 9000 - 90000 seconds </font></li><li>Comes to a few days (25 hours!)</font></li></ul></li><li>Crowd sourcing is also a good way to get data</font></li><li style="list-style: none; display: inline"><ul><li>Risk or reliability issues</font></li><li>Cost</font></li><li>Example</font></li><li style="list-style: none; display: inline"><ul><li>E.g. Amazon mechanical turks</font></li></ul></li></ul></li></ul><div style="font-size: 27px;"><b><u>Ceiling analysis: What part of the pipeline to work on next</font></u></b></div><ul style="font-size: 18px;"><li>Through the course repeatedly said one of the most valuable resources is developer time</font></li><li style="list-style: none; display: inline"><ul><li>Pick the right thing for you and your team to work on</font></li><li>Avoid spending a lot of time to realize the work was pointless in terms of enhancing performance</font></li></ul></li></ul><div style="font-size: 18px;"><b>Photo OCR pipeline</font></b></div><ul style="font-size: 18px;"><li>Three modules</font><ul><li>Each one could have a small team on it</font></li><li>Where should you allocate resources?</font></li></ul></li><li>Good to have a single real number as an evaluation metric</font><ul><li>So, character accuracy for this example </font></li><li>Find that our test set has 72% accuracy</font></li></ul></li></ul><b style="font-size: 18px;">Ceiling analysis on our pipeline</font></b><ul style="font-size: 18px;"><li>We go to the first module</font></li><li style="list-style: none; display: inline"><ul><li>Mess around with the test set - manually tell the algorithm where the text is</font></li><li>Simulate if your text detection system was 100% accurate</font><ul><li>So we're feeding the character segmentation module with 100% accurate data now</font></li></ul></li><li>How does this change the accuracy of the overall system<br><img src="18_Application_Example_OCR_files/Image [15].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font></li><li>Accuracy goes up to 89%</font></li></ul></li><li>Next do the same for the character segmentation</font><ul><li>Accuracy goes up to 90% now</font></li></ul></li><li>Finally doe the same for character recognition</font><ul><li>Goes up to 100%</font></li></ul></li><li>Having done this we can qualitatively show what the upside to improving each module would be</font><ul><li>Perfect text detection improves accuracy by 17%!</font><ul><li>Would bring the biggest gain if we could improve</font></li></ul></li><li>Perfect character segmentation would improve it by 1%</font><ul><li>Not worth working on</font></li></ul></li><li>Perfect character recognition would improve it by 10%</font><ul><li>Might be worth working on, depends if it looks easy or not</font></li></ul></li></ul></li><li>The &quot;ceiling&quot; is that each module has a ceiling by which making it perfect would improve the system overall</font></li></ul><div style="font-size: 18px;"><b>Other example - face recognition</font></b></div><ul style="font-size: 18px;"><li>NB this is not how it's done in practice<br><img src="18_Application_Example_OCR_files/Image [16].png" type="image/png" style="cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;cursor: default;"/></font><ul><li>Probably more complicated than is used in practice</font></li></ul></li><li>How would you do ceiling analysis for this</font><ul><li>Overall system is 85%</font></li><li>Perfect background -&gt; 85.1%</font><ul><li>Not a crucial step</font></li></ul></li><li>+ Perfect face detection -&gt; 91%</font><ul><li>Most important module to focus on</font></li></ul></li><li>+ Perfect eyes -&gt;95%</font></li><li>+ Perfect Nose -&gt; 96%</font></li><li>+ Perfect Mouth -&gt; 97%</font></li><li>+ Perfect logistic regression -&gt; 100%</font></li></ul></li><li>Cautionary tale</font><ul><li>Two engineers spent 18 months improving background pre-processing</font><ul><li>Turns out had no impact on overall performance</font></li><li>Could have saved three years of man power if they'd done ceiling analysis</font></li></ul></li></ul></li></ul></div></body></html> 