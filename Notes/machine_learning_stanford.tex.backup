\documentclass{article}
\title{Machine Learning Course Notes\\Stanford -- Coursera}
\date{29-12-2015}
\author{Bartol Freskura}

% packages
\usepackage{lmodern} % load a font with all the characters
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage[ruled]{algorithm2e}
\usepackage{geometry}
\usepackage{listings}
\geometry{legalpaper, portrait, margin=1in}
\usepackage{graphicx}
\graphicspath{ {images/} }

%Definite environments for lemmas, defintions and theorems
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\setlength{\parindent}{0pt}
\setlength{\parskip}{\baselineskip}

\begin{document}
% \renewcommand*\rmdefault{phv} %Font change

\pagenumbering{gobble}
\maketitle
\newpage


\tableofcontents
\pagenumbering{arabic}
\newpage

%Week 5

\section{Week 5}
\bigskip

\subsection{Cost Function and Backpropagation}

\bigskip

\subsubsection{Cost Function for neural networks}

\begin{align*}
&J(\Theta)=-\frac{1}{m}\left[\sum\limits_{i=1}^m \sum\limits_{k=1}^K =
			     y{_k^{(i)}}\log({h(x^{(i)})}) +
			     (1-y{_k^{(i)}})\log{(1-(h(x^{(i)}))_k)}\right]+
			     \frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum
			     \limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l+1}}
			     {\left(\theta{_{ji}^{(l)}} \right)}^2
\end{align*}

\subsubsection{Calculating cost error}

\begin{center}
$\delta{_j^{(L)}} = a{_j^{(L)}} - y_j$ --- error of node $j$ in layer $L$\\
$g'(z) = \frac{1}{1+e^{-z}}.*\left(1-\frac{1}{1+e^{-z}}\right)$ --- sigmoid function 
derivation\\
$\delta^{(L-1)} = (\theta^{{(L-1)}})^T\delta^{(L)}.*g'(z^{(L-1)}) $  --- error vector 
for level 
$L-1$
\end{center}

\subsubsection{Backpropagation algorithm}
\begin{algorithm}
Training set --- $\left\{(x^{(1)}, y^{(1)}),\dotsc, (x^{(m)}, y^{(m)})\right\}$\\
Set gradient --- $\Delta{_{ij}^{(l)}} = 0$ for all $l, i, j$\medskip\\
\For{$i=1$ to $m$}{
  
  Set $a^{(1)}=x^{(i)}$\smallskip
  
  Perform forward propagation to compute $a^{(l)}$ for $l=2,3,\dotsc,L$\smallskip
  
  Using $y^{(i)}$ compute $\delta^{(i)}=a^{(L)} - y^{(i)}$\smallskip
  
  Compute $\delta^{(L-1)}, \delta^{(L-2)}, \delta^{(L-3)}, \dotsc, \delta^{(2)}$\smallskip
  
  $\Delta{_{ij}^{(l)}} =  \Delta{_{ij}^{(l)}} +  a^{(l)}_j\delta^{(l+1)}_i$\smallskip
}
\caption{Backpropagation\label{IR}}
\end{algorithm}

\subsubsection{Gradient Checking}
Suppose you have a funciton $f_{i}(\theta)$ that computes 
$\frac{\partial}{\partial\theta_{i}}J(\theta)$; you'd like to check if $f_i$ is outputting correct 
derivative values.

\begin{align*}
\text{Let } \theta^{(i+)} = \theta +
\begin{bmatrix}
0\\
0\\
\vdots\\
\epsilon\\
\vdots\\
0\\
\end{bmatrix}
\indent\text{ and } \theta^{(i-)} = \theta -
\begin{bmatrix}
0\\
0\\
\vdots\\
\epsilon\\
\vdots\\
0\\
\end{bmatrix}
\end{align*}
You can now numerically verify $f_i(\theta)$â€™s correctness by checking, for each
$i$, that:
\begin{align*}
f_i(\theta)=\frac{J(\theta^{(i+)})-J(\theta^{(i-)})}{2\epsilon}
\end{align*}
\newpage
%Week 5




%Week 6
\section{Week 6}
\bigskip

\subsection{Evaluating a Learning Algorithm}
\medskip
What to do after you have implemented the algorithm and the algorithm gives bad results
\begin{itemize}
\item Get more training examples
\item Try smaller sets of features
\item Try gettting additional features
\item Try adding polynomial features $(x_1^2,x_2^2,x_1x_2, etc.)$
\item Try increasing or decreasing $\lambda$
\end{itemize}

\paragraph{Machine learning diagnostic:}
A test that you can run to gain insight what is or isn't 
working with a learning algorithm, and gain guidance as to how best to improve its performance

One way to evaluate your hypothesis is to split the training dataset into two groups.\newline
First group will contain 70\% of the training set and it will be called \emph{Training Set}, while
the other 30\% will be used as the \emph{Test Set}.\newline
We will denote test set examples as: $(x_{test}^{(i)},y_{test}^{(i)})$

\textbf{Testing procedure for linear regression}
\begin{itemize}
\item Learn paramater $\theta$ from trianing data (minimizing $J(\theta)$)
\item Compute test set error:
\begin{center}
$J_{test}(\theta) = 
\frac{1}{2*m_{test}}\sum\limits_{i=1}^{m_{test}}=\left(h_{\theta}(x^{(i)}_{test}) - 
y^{(i)}_{test}\right)$
\end{center}
\end{itemize}

\textbf{Testing procedure for logistic regression}
\begin{itemize}
\item Learn paramater $\theta$ from trianing data (minimizing $J(\theta)$)
\item Compute test set error:
\begin{center}
$J_{test}(\theta) = - \frac{1}{m_{test}}\sum\limits_{i=1}^{m_{test}} =    
y{_{test}^{(i)}}\log({h(x^{(i)}_{test})}) +
(1-y{_{test}^{(i)}})\log{h(x_{test}^{(i)})}$
\end{center}
\end{itemize}

\paragraph{Error with model selection with degree $d$:}
$J_{test}(\theta^{(j)})$ is likely to be an optimistic estimate of generilazation error. 
I.e. our extra parameter ($d$ = degree of polynomial) is fit to test set.
\paragraph{Solution to this problem:}
Split the data set into three subsets: \emph{Training set}(60\%), \emph{Cross validation 
set}(20\%) and \emph{Test set}(20\%)\newline
We will denote cross validation set examples as: $(x_{cv}^{(i)},y_{cv}^{(i)})$

Final procedure for model selection:
\begin{enumerate}
\item Choose the model that has the lowest $J_{cv}$, e.g. $J_{cv}(\theta^{(4)})$ --- 4 denotes 
the polynomial degree
\item Calculate the generalization error for test set $J_{test}(\theta^{(4)})$
\end{enumerate}

\subsection{Bias vs. Variance}
\paragraph{High Bias:} both $J_{train}(\theta)$ and  $J_{CV}(\theta)$ will be high. Also 
$J_{train}(\theta)\approx J_{CV}(\theta)$
\paragraph{High Variance:} $J_{train}(\theta)$ will be low and  $J_{CV}(\theta)$ will be much 
greater than  $J_{train}(\theta)$

A large $\lambda$ heavily penalizes all the $\theta$ parameters, which greatly simplifies the line 
of our resulting function, so causes underfitting.

The relationship of $\lambda$ to the training set and the variance set is as follows:
\begin{itemize}
\item Low $\lambda$: $J_{train}(\theta)$ is low and $J_{CV}(\theta)$  is high (high 
variance/overfitting).
\item Intermediate $\lambda$: $J_{train}(\theta)$ and $J_{CV}(\theta)$  are somewhat low and 
$J_{train}(\theta)\approx J_{CV}(\theta)$
\item Large $\lambda$: both $J_{train}(\theta)$ and $J_{CV}(\theta)$  will be high 
(underfitting/high bias)
\end{itemize}

\centerline{\includegraphics[scale=0.5]{features_poly}}

Our decision process can be broken down as follows:
\begin{itemize}
\item Getting more training examples --- Fixes high variance
\item Trying smaller sets of features --- Fixes high variance
\item Adding features --- Fixes high bias
\item Adding polynomial features --- Fixes high bias
\item Decreasing $\lambda$ --- Fixes high bias
\item Increasing $\lambda$ --- Fixes high variance
\end{itemize}

\subsubsection{Diagnosing neural networks}
A neural network with fewer parameters is prone to underfitting. It is also computationally 
cheaper.

A large neural network with more parameters is prone to overfitting. It is also computationally 
expensive. In this case you can use regularization (increase $\lambda$) to address the overfitting.

\subsection{Handling Skewed Data}
\paragraph{Precision:} $\frac{True\ positive}{True\ positive + False_positive}$
\paragraph{Recall:} $\frac{True_positive}{True_positive + False_negative}$











\newpage
%Week 6


\end{document}
